{"content": {"Introduction": {"para_sentences": {"para_0": "We have seen the hard problems out in the world that cannot be solved in polynomial time. But what do we do about them? There are two options: 1. Solve small input sizes of these problems by following exhaustive search, but it might not always work for us as the exhaustive search time complexity grows very fast. 2. Solve these problems and obtain an approximate solution.", "para_1": "In this section, we will look into the second option. We will explore some strategies that are followed to obtain approximate solutions for NP-Hard problems.", "para_2": "As you go through this section, try to answer the following questions:"}, "li_sentences": {"ul_0": ["What is meant by a heuristic?", "What is an approximation ratio?", "What are some strategies to implement NP-hard problems?"]}}, "Approximation Algorithms": {"para_sentences": {"para_0": "While we know that algorithms are step by step procedures to solve a problem, a heuristic is a simple strategy that we build mentally to form an approximate solution to a problem, it is not mathematical approach. We have already used one heuristic technique before. Can you think which one? Yes, the Greedy strategy. Using a greedy strategy we mentally run through possible ways of solving a problem and then pick one that appears to give us the near-optimal solution. Those algorithms that produce near-optimal solutions are called approximation algorithms.", "para_1": "C : The solution produced by the algorithm", "para_2": "C* : The optimal solution", "para_3": " is the approximation ratio for an approximation algorithm of problem size n.", "para_4": "\u00a0", "para_5": "If an algorithm has  approximation ratio, it is called -approximation algorithm.", "para_6": "To maintain a uniform scale we either perform \u00a0 or \u00a0 , to make the ratio greater than 1. If the scale is equal to 1 we can say that the algorithm produces the optimal solution.", "para_7": "For example, if we are computing a minimum optimal solution and if  = 1.5, then we can say that the approximation algorithm has computed a solution that is at most 50% larger than the optimal solution.", "para_8": "Problem: Given a graph, find the minimum set of vertices that cover all the edges of the graph.", "para_9": "One possible approach to solve this problem will be to pick a random edge E that has vertices (u,v). Add these vertices to the solution set.\u00a0Remove all edges that are connected to either u,v.\u00a0", "para_10": "\u00a0", "para_11": "Pseudocode for this approach.", "para_12": "Time complexity: The while loop will cover all edges E in O(E) time and we add each vertex in the worst case when we perform a union, this will make it the additional time of O(V). The total time complexity will be O(V+E). Which is a polynomial time.\u00a0", "para_13": "We will see that this solution has an approximation ratio of 2, which means that it guarantees a solution that is no more than twice the size of the optimal solution.\u00a0", "para_14": "In the algorithm, we are selecting a pair of vertices (u,v) that are not connected to any of the selected vertices. Let k denote the set of edges arbitrarily picked in the while loop.What is the relation between k and the optimal solution for this problem C* ? Any vertex cover (C* in particular) must include at least one endpoint of each edge in k. This is because no two edges in k share a vertex since we are removing all edges that are connected to picked edge (u,v). Thus, no two edges in k are covered by the same vertex from C*; and hence k give us a lower bound on C*.", "para_15": "", "para_16": "Each execution of picking an arbitrary edge (u,v) for which neither of the end points are already in C, gives us an upper bound on the size of vertex cover returned. We can say that the solution constructed by the approximation algorithm will have 2k vertices. This is because these k edges have no adjacent edges as we are removing the edges that are connected to these edges.\u00a0", "para_17": "\u00a0", "para_18": "\u00a0 \u00a0(from the equation)", "para_19": "", "para_20": "This gives us an approximation ratio of 2."}, "li_sentences": {}}, "Traveling Salesman Problem (TSP)": {"para_sentences": {"para_0": "The Traveling Salesman Problem is one of the most well-known NP-Complete problems. It has been intriguing scientists for more than a century. In simple language, the problem is: \"Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city?\".\u00a0", "para_1": "The below deck covers related terminology that we will use further and the TSP problem explanation pictorially.", "para_2": "We use a greedy approach to find the near-optimal solution for the TSP problem.", "para_3": "The simplest approach to solve the TSP problem is by employing the greedy technique. We start at an arbitrary vertex and we ask what is the closest vertex that we have not covered so far and travel towards it. This heuristic is called 'closest point heuristic' or 'nearest neighbor heuristic'.\u00a0", "para_4": "The steps we will cover:", "para_5": "Let us apply these steps on the below map.", "para_6": "\u00a0", "para_7": "We get the path: A->B-> C->D->A. This give us the cost of (1 + 2+ 1+ 7 ) = 11.", "para_8": "The optimal path is A->B->D->C->A. This path has a cost of (1 + 3+ 1+ 3) = 8.", "para_9": "The approximation ratio for these values:", "para_10": " = \u00a0=\u00a0\u00a0", "para_11": "This is a sample approximate value and this value can be large depending upon the value cost of (D, A) in this example.", "para_12": "Do you notice any similarity between the Hamiltonian circuit and the TSP problem? Basically, TSP is just asking for the minimum cost Hamiltonian circuit for a graph. We have seen the Minimum Spanning Tree (MST) in the previous modules. MST gives us the lower bound for the TSP problem. There is a close connection between MST and Hamiltonian Circuit problem. Removing an edge from the minimum-cost Hamiltonian circuit gives us an MST. There is an approach to solve TSP problem using these known techniques and using the triangle inequality.", "para_13": "1. We will find the MST for the graph, we can use any known MST algorithm for this. Let us use Prim's Algorithm.", "para_14": "2. Then we will use MST to create a walk using the vertices given by MST", "para_15": "3. Next we shall create a Hamiltonian cycle based on the path", "para_16": "This deck demonstrates the steps.", "para_17": "Note that this algorithm works for a fully connected graph.", "para_18": "Write pseudocode for this approach:", "para_19": "The time complexity of this algorithm is dominated by the MST calculation, which is , if we linearly search an array of weights to find the minimum weight edge to add to MST. Hence this algorithm runs in polynomial time.", "para_20": "We have seen that the TSP cost gives us the lower bound on the cost of the Hamiltonian cycle.", "para_21": "\u00a0", "para_22": "We have also seen in the deck that the cost of the walk that is built based on the MST is twice that of the TSP length, as we are visiting each edge twice.\u00a0", "para_23": "\u00a0", "para_24": "If the graph satisfies the triangle inequality, then we can say that when we are constructing our final solution which is the cycle made from the walk, the overall cost of the cycle will be less than that of the walk.", "para_25": "\u00a0", "para_26": "\u00a0", "para_27": "\u00a0 \u00a0 since ", "para_28": "\u00a0 \u00a0since ", "para_29": " = \u00a0", "para_30": "The approximation ratio for the TSP algorithm 2.", "para_31": "NP problems can be solved using approximation strategies. We have seen the backtracking technique and greedy technique in previous modules. We also looked at a few more greedy ways of solving problems in this section.", "para_32": "These approximation strategies have their limitations. For instance, using backtracking we can only solve small values of inputs.\u00a0"}, "li_sentences": {"ol_0": ["Pick an arbitrary starting point", "Pick next closest un-visited vertex", "Repeat until all the vertices are visited and return to the starting point"]}}, "Exercises": {"para_sentences": {"para_0": ""}, "li_sentences": {"ul_0": ["Implement the vertex cover problem that was discussed in this section"]}}, "Optional Additional Resources": {"para_sentences": {"para_0": "If this topic interests you, then you might find the following optional resources useful."}, "li_sentences": {"ul_0": ["CLRS section 35", "CLRS section 35"]}}}, "heading_sentences": ["Exploration 9", "3: Approximation Algorithms to Solve NP-Hard Problems"], "__pre_h2__": {"pre_h2_para_sentences": {}, "pre_h2_li_sentences": {}}, "content_links": {"ul_0": [[]], "ol_0": [[]]}, "preh2_links": {}}